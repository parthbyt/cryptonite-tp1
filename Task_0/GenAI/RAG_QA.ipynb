{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline on Wikipedia Dataset"
      ],
      "metadata": {
        "id": "X3UBbvjUHF45"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux27y3HbGaOp",
        "outputId": "2bad0761-45df-49b0-ecc9-e587a52a48ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.12/dist-packages (0.1.22)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from opendatasets) (4.67.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from opendatasets) (1.7.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from opendatasets) (8.3.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.3.7)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2026.1.4)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.32.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence_transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets faiss-cpu sentence_transformers\n",
        "import nltk\n",
        "import faiss\n",
        "import numpy as np\n",
        "import re\n",
        "import opendatasets as od\n",
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/ffatty/plain-text-wikipedia-simpleenglish')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo185jybIdZS",
        "outputId": "ca2094f3-c561-4bec-ee0e-9002d1a7cd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./plain-text-wikipedia-simpleenglish\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking the text"
      ],
      "metadata": {
        "id": "Un7d10L9JALX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('plain-text-wikipedia-simpleenglish/AllCombined.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoIbZ9noIkZa",
        "outputId": "de1fc30b-c806-446a-924b-08d51709f040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, dtype=torch.float32)\n",
        "\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "eb9dcde479314502997abecc0550a745",
            "85298820b5fd4fc9acac88d94ac4ea6a",
            "07e0c2f6cc9949d88504349e5e6c7d6b",
            "6e037faf7fd5489a829b26278b763d8e",
            "1ed9c1db55144a848df6c51228183e48",
            "9d5d515b519a43a9a871d848532ab6f9",
            "da2630021fca442fa0ca59a62ef3e777",
            "1d40eedbef10437a8ef3108ad2a0abfa",
            "9486ec21b3e84f6fbf6bea8fba4763b6",
            "3c7f007d4ead4cdd8540888e0eeb62eb",
            "3bf3e463e74949cca5e228ba51f63768"
          ]
        },
        "id": "9q4BF03QvlQM",
        "outputId": "d739ee3a-adaa-467a-ad37-4cd4a38d29b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb9dcde479314502997abecc0550a745"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "chunks, current = [], []\n",
        "token_count = 0\n",
        "\n",
        "min_tokens = 200\n",
        "max_tokens = 300\n",
        "\n",
        "for sent in sentences:\n",
        "  sent_tokens = len(tokenizer.encode(sent, add_special_tokens=False))\n",
        "\n",
        "  if sent_tokens > max_tokens:\n",
        "      continue\n",
        "\n",
        "  if token_count + sent_tokens > max_tokens:\n",
        "      if token_count >= min_tokens:\n",
        "          chunks.append(\" \".join(current))\n",
        "      current = [sent]\n",
        "      token_count = sent_tokens\n",
        "  else:\n",
        "      current.append(sent)\n",
        "      token_count += sent_tokens\n",
        "\n",
        "if token_count >= min_tokens:\n",
        "    chunks.append(\" \".join(current))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u76xWle2JR8o",
        "outputId": "31d78855-8d9e-4c33-b25f-07ebe02136a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3553 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Retriever"
      ],
      "metadata": {
        "id": "DUktmwSAKsYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "472a65d3b78348ebbc05a2bfffad9a1c",
            "7724d21cfa0144789666cc6f52c999c2",
            "59c4d0c4e3f84907a4173bea683c3b78",
            "fdf784b756fc4a549aaad8564e79f28d",
            "e00ebef9d32b4500aceb509d4f805371",
            "5e31dd9a430843d99c1118874c967117",
            "74bc098c2927411fb93e8088fc483028",
            "63e3786c95e04b2dac2e5bc4c813236c",
            "d00aa4baf86746baaa1dee2aa8b081ff",
            "e582e47d7f774a65a5540ddcf0438e56",
            "88e56ecd087942d6867228d434130325"
          ]
        },
        "id": "sncjnzQVKvJA",
        "outputId": "e022e241-a840-4704-a195-d19eecfc20e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "472a65d3b78348ebbc05a2bfffad9a1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embedder.encode(chunks)\n",
        "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "index.add(embeddings)"
      ],
      "metadata": {
        "id": "PZVGB1xoK4kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=3, min_sim=0.3):\n",
        "  query_embedding = embedder.encode([query])\n",
        "  scores, idx = index.search(query_embedding, k)\n",
        "\n",
        "  results = []\n",
        "  for score, i in zip(scores[0], idx[0]):\n",
        "    if score >= min_sim:\n",
        "       results.append(chunks[i])\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "LfoiTVUALPUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information Check"
      ],
      "metadata": {
        "id": "WkAjhbVfXRYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def info_check(retrieved):\n",
        "  return len(retrieved) > 0"
      ],
      "metadata": {
        "id": "EduZWmRMc5a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Answerer LLM"
      ],
      "metadata": {
        "id": "Irv3CQv3LzdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_answer(prompt):\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "  inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "  input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=150,\n",
        "      do_sample=False,\n",
        "  )\n",
        "\n",
        "  generated_tokens = outputs[0]\n",
        "\n",
        "  return tokenizer.decode(generated_tokens, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "FtO441xcL3GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post Processing"
      ],
      "metadata": {
        "id": "LjqGiU2fXNgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process(text):\n",
        "    # Remove parentheses\n",
        "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
        "\n",
        "    # Fix spacing\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    merged = []\n",
        "    for s in sentences:\n",
        "        if merged and len(s.split()) < 6:\n",
        "            merged[-1] += \" \" + s\n",
        "        else:\n",
        "            merged.append(s)\n",
        "\n",
        "    # Enforce 2â€“3 sentences\n",
        "    merged = merged[:3]\n",
        "\n",
        "    return \" \".join(merged)\n"
      ],
      "metadata": {
        "id": "utc7Gwkiln8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "Slt82-CLc92Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_context(chunks, tokenizer, max=1500):\n",
        "  kept = []\n",
        "  total = 0\n",
        "\n",
        "  for c in chunks:\n",
        "    tokens = tokenizer.encode(c)\n",
        "    if total + len(tokens) > max:\n",
        "      break\n",
        "    kept.append(c)\n",
        "    total += len(tokens)\n",
        "\n",
        "  return kept"
      ],
      "metadata": {
        "id": "cdh5qLxFiYhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question):\n",
        "    retrieved = retrieve(question, 3, 0.3)\n",
        "    retrieved = truncate_context(retrieved, tokenizer, 500)\n",
        "\n",
        "    if not info_check(retrieved):\n",
        "        return \"Not enough information in the Simple Wikipedia dataset.\"\n",
        "\n",
        "    context = \"\\n\".join(retrieved)\n",
        "    prompt = f\"\"\"\n",
        "    Use only the text below to answer the question.\n",
        "Do not add new facts.\n",
        "If the text does not answer the question, say:\n",
        "\"Not enough information in the Simple Wikipedia dataset.\"\n",
        "\n",
        "Text:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\n",
        "    \"\"\"\n",
        "    raw = gen_answer(prompt)\n",
        "    final = post_process(raw)\n",
        "\n",
        "\n",
        "    return final\n"
      ],
      "metadata": {
        "id": "9AdPr7h7dBDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "oAekIFGu67xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working check"
      ],
      "metadata": {
        "id": "8CHFUA_569yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is capital of France?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CvTKW6BbdPR9",
        "outputId": "2709c8a1-8d19-4f50-ae0c-56f0a5896530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type 1: Single chunk Factual Question\n",
        "Hard coded simple fact based questions."
      ],
      "metadata": {
        "id": "XCcJqE2Q7Ccg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is water?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GZngANdZ6dFo",
        "outputId": "6d65a1ed-f556-4020-f50f-b553caddcd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'molecule made of two hydrogen atoms and one oxygen atom.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is sun?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e2jw9Ysx6hF6",
        "outputId": "fd6ffae9-1567-4139-f94c-0ebc7d475907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A star like many others in our Milky Way galaxy. The Sun is a type of star called a G-type main-sequence star based on its spectral class. The Sun has existed for a little over 4.5 billion years.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is mount_everest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9MWlgvPB6lP7",
        "outputId": "4e03cbc1-60b6-4cc1-b297-3477febecdac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mount Everest is the highest mountain on Earth.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type 2: Rewriting Check\n",
        "Ensure that model isn't copying text."
      ],
      "metadata": {
        "id": "MlSX9VNH7Rhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Why are bees important?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nsP9AwDP63Wk",
        "outputId": "eb4a1df8-3902-4fcd-857b-708d24f102a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Pollinators for many plants.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is electricity?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "T3z42mmV7bMz",
        "outputId": "90b8d356-bd8b-4c00-fda6-9147691ae81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Electrical energy is mostly generated in places called power stations.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type 3: Multi Fact Questions"
      ],
      "metadata": {
        "id": "DqEvWge37qHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What was World War II and when was it fought??\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LgA6mkUV7fPL",
        "outputId": "edb0ba8c-2bf3-4d0e-8046-a5d2db560fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'World War II began in 1939.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who were alies in World War II? Name countires.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DqJ1OZ4l72Rc",
        "outputId": "fd8d6595-5c36-4aa5-a0a0-9387f3f6cfea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Russia, France, the British Empire and later the United States.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It struggles a little with multi-fact questions."
      ],
      "metadata": {
        "id": "1t63rcag8JMf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type 4: Refusal Test.\n",
        "Check whether it answers correctly or not for data not in dataset."
      ],
      "metadata": {
        "id": "Jk9wQPyO8MsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who was the president of United States in 2023?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jQlK2bJX7-HT",
        "outputId": "dfcf10b4-45db-420a-f527-8b72ebb06725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Not enough information in the Simple Wikipedia dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What is population of Mars?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UTxpbCaG8gO8",
        "outputId": "c49289cf-c09a-4b14-cce3-99dc9c873ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Not enough information in the Simple Wikipedia dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type 5: Guess Test\n",
        "\n",
        "We will check if model guesses or not."
      ],
      "metadata": {
        "id": "rDxZRi2B8uwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"Who won World War III?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9YBZf1nj820L",
        "outputId": "0de2d5e8-0c94-4c82-de0f-50b644f47060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The United States and Western Europe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_question(\"What did Isacc Newton tweet?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Kc-wBI--85pY",
        "outputId": "33cf1d15-e9f0-4a65-b02f-2aac17422327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Not enough information in the Simple Wikipedia dataset.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It still guesses some of the information."
      ],
      "metadata": {
        "id": "kA3I4XwX9Ccb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "The RAG Pipeline performs really well on most queries and reverts when information is not present. Overall we find a great implemetation of the whole work."
      ],
      "metadata": {
        "id": "vp1IOWx99kl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Questions\n",
        "\n",
        "**Q.1** How often does the agent hallucinate facts not present in retrieved text?\n",
        "\n",
        "**Ans.** As clear from type 5 questions, if the data is absurd like 'What did Isaac Newton tweet?', It'll not hallucinate, but if the data closer to real life 'like World War III' which might had some arbiratory reference in text, the model has higher chance of hallucinating.\n",
        "Also as shown by type 4 questions, being asked for explicit answers (like population of mars or name of president in 2023) the model has much higher chance of not guessing.\n",
        "\n",
        "**Q.2** How sensitive is answer quality to retrieval errors?\n",
        "\n",
        "**Ans.** It is quite specific fro small models like flan-t5-small, where it tended to treat wrong retrievels as only truth (like if there was any city name it would classify it as capital of paris without knowing whether it really was capital related or just a random drop.). However when we switch to larger models like flan-t5-base, we observed that it better understood the grammatical structure of test and could differentiate between values.\n",
        "\n",
        "**Q.3** How does reliability change across 25M, 80M, and 250M parameter models?\n",
        "\n",
        "**Ans.** It changes significantly. Although I could not find an appropriate 25M model with a context window enough to run the model, I tried with faln-t5-small with 80M parameter. The major change across reliability was relying on context size change since 512 token model was not able to get enough retrieved text to generate accurately and chose to stay silent.\n",
        "\n",
        "**Q.4** Which matters more: strict prompting or rule-based post-editing?\n",
        "\n",
        "**Ans.** Based on my observations, strict prompting is much more important than post editing. In this case providing model with a prompt to refuse was much more helpful than trying to implement it via rule based editing. As the model sizes imporved, I could achieve better grammar and reduced depenendence on post based editing.\n",
        "\n",
        "**Q.5** How accurately does the agent refuse when information is missing?\n",
        "\n",
        "**Ans.** As shown by types 5 and 4, the agent will refuse if it does not have concrete facts but if something vaguely related or has mentions like 'World War 3' it will try to guess/hallucinate."
      ],
      "metadata": {
        "id": "JoGPVV3ukLFW"
      }
    }
  ]
}